{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning Using LoRA and QLoRA\n",
    "\n",
    "This script demonstrates fine-tuning the LLaMA-2 model using LoRA (Low-Rank Adaptation) and QLoRA (Quantized Low-Rank Adaptation). \n",
    "It performs:\n",
    "1. Dataset loading and preparation.\n",
    "2. Model configuration with quantization.\n",
    "3. Fine-tuning using SFTTrainer.\n",
    "4. Saving and reloading the fine-tuned model.\n",
    "5. Example usage with text generation.\n",
    "\n",
    "Dependencies:\n",
    "- accelerate\n",
    "- peft\n",
    "- bitsandbytes\n",
    "- transformers\n",
    "- trl\n",
    "\n",
    "**Note:**\n",
    " - For QLoRA training, we will utilize a rank of 64 and a scaling parameter of 16. The Llama 2 model will be loaded in 4-bit precision using the NF4 data type. Training will be conducted for a single epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Required Packages\n",
    "!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "model_name = \"NousResearch/Llama-2-7b-chat-hf\"  # Base model name\n",
    "new_model = \"Llama-2-7b-chat-finetune\"         # Output fine-tuned model name\n",
    "dataset_name = \"mlabonne/guanaco-llama2-1k\"    # Dataset name\n",
    "\n",
    "# LoRA Parameters\n",
    "lora_r = 64\n",
    "lora_alpha = 16\n",
    "lora_dropout = 0.1\n",
    "\n",
    "# Quantization Parameters\n",
    "use_4bit = True\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "use_nested_quant = False\n",
    "\n",
    "# Training Arguments\n",
    "training_args = {\n",
    "    \"output_dir\": \"./results\",\n",
    "    \"num_train_epochs\": 1,\n",
    "    \"per_device_train_batch_size\": 4,\n",
    "    \"gradient_accumulation_steps\": 1,\n",
    "    \"optim\": \"paged_adamw_32bit\",\n",
    "    \"save_steps\": 0,\n",
    "    \"logging_steps\": 25,\n",
    "    \"learning_rate\": 2e-4,\n",
    "    \"weight_decay\": 0.001,\n",
    "    \"fp16\": False,\n",
    "    \"bf16\": False,\n",
    "    \"max_grad_norm\": 0.3,\n",
    "    \"max_steps\": -1,\n",
    "    \"warmup_ratio\": 0.03,\n",
    "    \"group_by_length\": True,\n",
    "    \"lr_scheduler_type\": \"cosine\",\n",
    "    \"report_to\": \"tensorboard\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "dataset = load_dataset(dataset_name, split=\"train\")\n",
    "\n",
    "# Quantization Configuration\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")\n",
    "\n",
    "# GPU Compatibility Check\n",
    "if compute_dtype == torch.float16 and use_4bit:\n",
    "    major, _ = torch.cuda.get_device_capability()\n",
    "    if major >= 8:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "# Load Model and Tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map={\"\": 0},\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "# Tokenizer Configuration\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# LoRA Configuration\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_r,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Training Arguments\n",
    "training_arguments = TrainingArguments(**training_args)\n",
    "\n",
    "# Fine-Tuning Setup\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing=False,\n",
    ")\n",
    "\n",
    "# Train Model\n",
    "trainer.train()\n",
    "\n",
    "# Save Trained Model\n",
    "trainer.model.save_pretrained(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Generation Example\n",
    "logging.set_verbosity(logging.CRITICAL)\n",
    "prompt = \"What is a large language model?\"\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean Up VRAM\n",
    "del model, pipe, trainer\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge and Save LoRA Model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map={\"\": 0},\n",
    ")\n",
    "model = PeftModel.from_pretrained(base_model, new_model)\n",
    "model = model.merge_and_unload()\n",
    "tokenizer.save_pretrained(new_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
